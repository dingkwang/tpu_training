{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n\"\"\"\nMBPP+ GRPO Training on Kaggle TPU\nTrain Gemma-3-1B on MBPP+ code generation with 10 samples.\n\"\"\"\n\nimport os\nimport sys\n\nprint(\"=\" * 80)\nprint(\"MBPP+ GRPO Training - 10 Samples\")\nprint(\"=\" * 80)\n\n# Step 1: Download MBPP+ dataset\nprint(\"\\n[1/6] Downloading MBPP+ dataset...\")\nimport subprocess\nresult = subprocess.run(\n    [\"pip\", \"install\", \"-q\", \"datasets\"],\n    capture_output=True,\n    text=True,\n    timeout=60\n)\nprint(\"  ✓ datasets library installed\")\n\n# Download dataset\nfrom datasets import load_dataset\nprint(\"  Downloading evalplus/mbppplus...\")\nos.makedirs(\"./data/mbppplus_hf\", exist_ok=True)\ndataset = load_dataset(\"evalplus/mbppplus\", split=\"test\")\ndataset.to_parquet(\"./data/mbppplus_hf/test.parquet\")\nprint(f\"  ✓ Downloaded {len(dataset)} samples\")\n\n# Step 2: Import libraries\nprint(\"\\n[2/6] Importing libraries...\")\nimport jax\nfrom flax import nnx\nimport optax\n\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\nfrom tunix.rl.rollout import base_rollout\nfrom huggingface_hub import snapshot_download\n\nfrom mbpp_data_loader import get_mbpp_dataset\nfrom reward_functions_mbpp import DEFAULT_REWARD_FNS_MBPP\nfrom config import *\n\nprint(\"  ✓ All libraries imported\")\n\n# Step 3: Detect TPU and create mesh\nprint(\"\\n[3/6] Setting up TPU mesh...\")\ndevices = jax.devices()\ndevice_type = devices[0].platform\nnum_devices = len(devices)\nprint(f\"  Device type: {device_type}\")\nprint(f\"  Number of devices: {num_devices}\")\n\nif num_devices == 8:\n    # TPU v3-8: reshape to 2D (1, 8)\n    print(f\"  Using 2D mesh: (1, 8)\")\n    import numpy as np\n    devices_2d = np.array(devices).reshape(1, 8)\n    mesh = jax.sharding.Mesh(\n        devices_2d,\n        axis_names=('fsdp', 'tp')\n    )\nelif num_devices == 1:\n    # Single device: use 2D mesh (1, 1)\n    print(f\"  Using 2D mesh (1, 1) for single device\")\n    import numpy as np\n    devices_2d = np.array(devices).reshape(1, 1)\n    mesh = jax.sharding.Mesh(\n        devices_2d,\n        axis_names=('fsdp', 'tp')\n    )\nelse:\n    # Use config.MESH\n    mesh = jax.make_mesh(\n        *MESH,\n        axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0])\n    )\n\nprint(f\"  ✓ Mesh created: {mesh}\")\n\n# Step 4: Load dataset (10 samples)\nprint(\"\\n[4/6] Loading 10 MBPP+ samples...\")\ntrain_dataset, val_dataset, test_dataset, dataset_lengths = get_mbpp_dataset(\n    local_path=\"./data/mbppplus_hf\",\n    train_fraction=1.0,\n    batch_size=1,\n    num_train_batches=10,\n    num_test_batches=2,\n    num_epochs=1,\n    shuffle=False,\n)\nprint(f\"  ✓ Loaded {dataset_lengths[0]} training batches\")\n\n# Step 5: Load model\nprint(\"\\n[5/6] Loading Gemma-3-1B model...\")\nmodel_config = gemma_lib.ModelConfig.gemma3_1b_it()\n\n# Download model\nprint(\"  Downloading from Hugging Face...\")\nlocal_model_path = snapshot_download(\n    repo_id=MODEL_ID,\n    ignore_patterns=[\"*.pth\"]\n)\nprint(f\"  Model at: {local_model_path}\")\n\n# Create model from safetensors\nprint(\"  Creating model on TPU mesh...\")\nwith mesh:\n    actor_model = params_safetensors_lib.create_model_from_safe_tensors(\n        local_model_path, model_config, mesh\n    )\nprint(\"  ✓ Model loaded\")\n\n# Create tokenizer\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\ntokenizer = tokenizer_lib.Tokenizer(\n    tokenizer_path=TOKENIZER_PATH,\n    tokenizer_type='sentencepiece'\n)\nprint(\"  ✓ Tokenizer loaded\")\n\n# Create optimizer and cluster config\noptimizer = optax.adamw(learning_rate=LEARNING_RATE)\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=10,\n        max_steps=10,\n        mini_batch_size=1,\n        train_micro_batch_size=1,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_SEQ_LEN,\n        temperature=TEMPERATURE,\n        top_k=TOP_K,\n        top_p=TOP_P,\n    ),\n)\n\n# Create RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=actor_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\nprint(\"  ✓ RL Cluster created\")\n\n# Step 6: Create GRPO trainer and run\nprint(\"\\n[6/6] Running GRPO training on 10 samples...\")\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n    num_iterations=NUM_ITERATIONS,\n)\n\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=DEFAULT_REWARD_FNS_MBPP,\n    algo_config=grpo_config,\n)\nprint(f\"  Config: {NUM_GENERATIONS} generations, beta={BETA}, epsilon={EPSILON}\")\nprint(f\"  Reward functions: {len(DEFAULT_REWARD_FNS_MBPP)}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Starting training...\")\nprint(\"=\" * 80)\n\ntry:\n    grpo_trainer.train(\n        train_ds=train_dataset,\n        eval_ds=test_dataset,\n    )\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"✅ Training completed successfully!\")\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"❌ Training failed: {e}\")\n    print(\"=\" * 80)\n    import traceback\n    traceback.print_exc()\n    sys.exit(1)\n","metadata":{"_uuid":"4e971e02-993e-4741-8952-51f7aa4e0735","_cell_guid":"546bec0c-6e94-49c1-ae5b-72e49ada0534","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-25T20:56:51.204441Z","iopub.execute_input":"2025-12-25T20:56:51.204764Z","iopub.status.idle":"2025-12-25T20:56:54.576384Z","shell.execute_reply.started":"2025-12-25T20:56:51.204742Z","shell.execute_reply":"2025-12-25T20:56:54.574985Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nMBPP+ GRPO Training - 10 Samples\n================================================================================\n\n[1/6] Downloading MBPP+ dataset...\n  ✓ datasets library installed\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ✓ datasets library installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Download dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Downloading evalplus/mbppplus...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33m./data/mbppplus_hf\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/__init__.py:22\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[32m     20\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m2.14.4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/arrow_writer.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpq\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     FeatureType,\n\u001b[32m     30\u001b[39m     _ArrayXDExtensionType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     to_pyarrow_listarray,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/features/__init__.py:18\u001b[39m\n\u001b[32m      3\u001b[39m __all__ = [\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAudio\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mArray2D\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTranslationVariableLanguages\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m ]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranslation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Translation, TranslationVariableLanguages\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/features/features.py:634\u001b[39m\n\u001b[32m    630\u001b[39m     \u001b[38;5;66;03m# Automatically constructed\u001b[39;00m\n\u001b[32m    631\u001b[39m     _type: \u001b[38;5;28mstr\u001b[39m = field(default=\u001b[33m\"\u001b[39m\u001b[33mArray5D\u001b[39m\u001b[33m\"\u001b[39m, init=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mrepr\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ArrayXDExtensionType\u001b[39;00m(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyExtensionType\u001b[49m):\n\u001b[32m    635\u001b[39m     ndims: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape: \u001b[38;5;28mtuple\u001b[39m, dtype: \u001b[38;5;28mstr\u001b[39m):\n","\u001b[31mAttributeError\u001b[39m: module 'pyarrow' has no attribute 'PyExtensionType'"],"ename":"AttributeError","evalue":"module 'pyarrow' has no attribute 'PyExtensionType'","output_type":"error"}],"execution_count":23}]}