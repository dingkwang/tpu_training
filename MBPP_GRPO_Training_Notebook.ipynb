{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpuV5e8",
      "dataSources": [],
      "dockerImageVersionId": 31235,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "MBPP GRPO Training Notebook",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6398f94d97cd40ea956e6faba3571248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0404b011e26a45bda3a343c933de7f15",
              "IPY_MODEL_b440f4107759428bb87a1387ca965dbc",
              "IPY_MODEL_7082feafc8914f0d902a10151435e5ee"
            ],
            "layout": "IPY_MODEL_a291e67142da4349abe852d10a703c43"
          }
        },
        "0404b011e26a45bda3a343c933de7f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7d19c857ef442d9a127a1ba8bd7395c",
            "placeholder": "​",
            "style": "IPY_MODEL_faa3b1d70b3047ecb4278097d35f66d7",
            "value": "data/test-00000-of-00001-d5781c9c51e0279(…): 100%"
          }
        },
        "b440f4107759428bb87a1387ca965dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee0f09ad4f14e01add55b75d157e1df",
            "max": 1129135,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1928f9b91d0e4a34aea3c2838abe629e",
            "value": 1129135
          }
        },
        "7082feafc8914f0d902a10151435e5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16babec0f8a94e148bf4ac5bd6f52274",
            "placeholder": "​",
            "style": "IPY_MODEL_12699d8a8fb8423081aff21afaecc0d1",
            "value": " 1.13M/1.13M [00:00&lt;00:00, 1.66MB/s]"
          }
        },
        "a291e67142da4349abe852d10a703c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d19c857ef442d9a127a1ba8bd7395c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa3b1d70b3047ecb4278097d35f66d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ee0f09ad4f14e01add55b75d157e1df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1928f9b91d0e4a34aea3c2838abe629e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16babec0f8a94e148bf4ac5bd6f52274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12699d8a8fb8423081aff21afaecc0d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6fa2dcb3583422e9dc0da6d887cf5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_938999d492ab424b8c2882ad633e5b18",
              "IPY_MODEL_c067a39f7c5e4db6b5063aa2dee1dd55",
              "IPY_MODEL_e768ba6dd1fc48488379d41040bd8bee"
            ],
            "layout": "IPY_MODEL_0f1f934ad21d4ca984d1c0d8e7d727b0"
          }
        },
        "938999d492ab424b8c2882ad633e5b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f43875ec228459fabf516ccebe0279f",
            "placeholder": "​",
            "style": "IPY_MODEL_4c2dc85576fd48a09109e8c63fc59eca",
            "value": "Generating test split: 100%"
          }
        },
        "c067a39f7c5e4db6b5063aa2dee1dd55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad848d1d23b4a268edecf3966639cc6",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d5a914c980f4090a21de68ac55c471b",
            "value": 378
          }
        },
        "e768ba6dd1fc48488379d41040bd8bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29622b40becb439da38dc095f059e804",
            "placeholder": "​",
            "style": "IPY_MODEL_b4a19eb0f7694c40b64ed5483a4d33a9",
            "value": " 378/378 [00:00&lt;00:00, 21291.46 examples/s]"
          }
        },
        "0f1f934ad21d4ca984d1c0d8e7d727b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f43875ec228459fabf516ccebe0279f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c2dc85576fd48a09109e8c63fc59eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ad848d1d23b4a268edecf3966639cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5a914c980f4090a21de68ac55c471b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29622b40becb439da38dc095f059e804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a19eb0f7694c40b64ed5483a4d33a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8fc998405ec45ceb944bdb97829dea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff81898aea14fd2b1dc5972e22efdc3",
              "IPY_MODEL_4547027b20b6455b8ed858b882c3b82f",
              "IPY_MODEL_3c8a484e3ff44f709db101fd416ec1b0"
            ],
            "layout": "IPY_MODEL_d5fcb5578cf545ac99874d602e4e866d"
          }
        },
        "eff81898aea14fd2b1dc5972e22efdc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ae139504d9a4f5491761afa0471d085",
            "placeholder": "​",
            "style": "IPY_MODEL_4b6fbfac189a4c13a43040cf5a83b483",
            "value": "Creating parquet from Arrow format: 100%"
          }
        },
        "4547027b20b6455b8ed858b882c3b82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9fca2f59bc54a71aaf97303139a4162",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41085ded10304cd791f2b03dbeb78e6c",
            "value": 1
          }
        },
        "3c8a484e3ff44f709db101fd416ec1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5e5f750d44459599f986fe609b95b1",
            "placeholder": "​",
            "style": "IPY_MODEL_34c332537e554aaebf859860f4f6eaa6",
            "value": " 1/1 [00:00&lt;00:00, 72.68ba/s]"
          }
        },
        "d5fcb5578cf545ac99874d602e4e866d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae139504d9a4f5491761afa0471d085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b6fbfac189a4c13a43040cf5a83b483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9fca2f59bc54a71aaf97303139a4162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41085ded10304cd791f2b03dbeb78e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c5e5f750d44459599f986fe609b95b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c332537e554aaebf859860f4f6eaa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dingkwang/tpu_training/blob/master/MBPP_GRPO_Training_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MBPP+ GRPO Training - All in One Notebook\n",
        "Complete pipeline for training Gemma-3-1B on MBPP+ code generation using GRPO."
      ],
      "metadata": {
        "id": "O6r9I3hv6UoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "AEPGZGyq6UoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "!pip install -q -U \"datasets==4.4.2\" \"numpy==2.0.2\" \"pyarrow==22.0.0\"\n",
        "!pip install -q -U \"google-tunix[prod]==0.1.5\"\n",
        "print(\"✓ All dependencies installed\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zIlRxfD-6UoG",
        "outputId": "adb900bb-1e8b-4238-a15b-8ca5c8e0bf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-metrax 0.2.4 requires numpy>=2.1.3, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ All dependencies installed\n",
            "CPU times: user 493 ms, sys: 87.2 ms, total: 580 ms\n",
            "Wall time: 7.18 s\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import datasets\n",
        "print(\"python\", sys.version)\n",
        "print(\"executable\", sys.executable)\n",
        "print(\"datasets\", datasets.__version__)\n",
        "# from datasets import load_dataset\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T04:00:33.842645Z",
          "iopub.execute_input": "2025-12-26T04:00:33.842875Z",
          "iopub.status.idle": "2025-12-26T04:00:35.181512Z",
          "shell.execute_reply.started": "2025-12-26T04:00:33.842857Z",
          "shell.execute_reply": "2025-12-26T04:00:35.180367Z"
        },
        "id": "XD9NPl8e6UoH",
        "outputId": "831a1346-7d17-4016-81fd-65deaa4fe840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "executable /usr/bin/python3\n",
            "datasets 4.4.2\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Configuration"
      ],
      "metadata": {
        "id": "WDGTSrmS6UoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
        "MAX_SEQ_LEN = 1024\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "LEARNING_RATE = 3e-6\n",
        "NUM_GENERATIONS = 2\n",
        "TEMPERATURE = 0.9\n",
        "TOP_K = 50\n",
        "TOP_P = 1.0\n",
        "BETA = 0.08\n",
        "EPSILON = 0.2\n",
        "NUM_ITERATIONS = 1\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "\n",
        "print(\"✓ Configuration loaded\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T01:32:31.270821Z",
          "iopub.execute_input": "2025-12-26T01:32:31.271076Z",
          "iopub.status.idle": "2025-12-26T01:32:31.275439Z",
          "shell.execute_reply.started": "2025-12-26T01:32:31.271055Z",
          "shell.execute_reply": "2025-12-26T01:32:31.274535Z"
        },
        "id": "K2DgAkn96UoI",
        "outputId": "09d7799d-5616-45a8-a601-4809670f9a3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Helper Functions"
      ],
      "metadata": {
        "id": "jLp0L1rO6UoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import subprocess\n",
        "from typing import List\n",
        "\n",
        "# ============================================================================\n",
        "# Reward Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_code(completion: str) -> str | None:\n",
        "    \"\"\"Extract Python code from model completion.\"\"\"\n",
        "    # Try ```python ... ```\n",
        "    python_block = re.search(r'```python\\s*\\n(.*?)\\n```', completion, re.DOTALL)\n",
        "    if python_block:\n",
        "        return python_block.group(1).strip()\n",
        "\n",
        "    # Try ``` ... ```\n",
        "    generic_block = re.search(r'```\\s*\\n(.*?)\\n```', completion, re.DOTALL)\n",
        "    if generic_block:\n",
        "        return generic_block.group(1).strip()\n",
        "\n",
        "    # Look for function definition\n",
        "    if 'def ' in completion:\n",
        "        def_start = completion.find('def ')\n",
        "        if def_start != -1:\n",
        "            return completion[def_start:].strip()\n",
        "\n",
        "    return None\n",
        "\n",
        "def has_code_block(completion: str) -> bool:\n",
        "    \"\"\"Check if completion contains a code block.\"\"\"\n",
        "    return bool(re.search(r'```(?:python)?\\s*\\n.*?\\n```', completion, re.DOTALL))\n",
        "\n",
        "def execute_test(code: str, test: str, test_imports, timeout: float = 3.0) -> tuple[int, int]:\n",
        "    \"\"\"Execute MBPP+ tests on generated code.\"\"\"\n",
        "    try:\n",
        "        script_parts = []\n",
        "\n",
        "        # Add imports\n",
        "        if test_imports:\n",
        "            if isinstance(test_imports, str):\n",
        "                if test_imports.strip():\n",
        "                    script_parts.append(test_imports)\n",
        "            else:\n",
        "                for imp in test_imports:\n",
        "                    script_parts.append(imp)\n",
        "\n",
        "        # Add code and test\n",
        "        script_parts.append(code)\n",
        "        script_parts.append(test)\n",
        "        full_script = '\\n'.join(script_parts)\n",
        "\n",
        "        # Execute\n",
        "        result = subprocess.run(\n",
        "            [\"python3\", \"-c\", full_script],\n",
        "            timeout=timeout,\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        test_count = test.count('assert')\n",
        "        if result.returncode == 0:\n",
        "            return (test_count, test_count)\n",
        "        else:\n",
        "            return (0, test_count if test_count > 0 else 1)\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        test_count = test.count('assert')\n",
        "        return (0, test_count if test_count > 0 else 1)\n",
        "    except Exception:\n",
        "        test_count = test.count('assert') if test else 1\n",
        "        return (0, test_count if test_count > 0 else 1)\n",
        "\n",
        "def mbppplus_verifier_reward(prompts: List[str], completions: List[str],\n",
        "                              test, test_imports=None, **kwargs) -> List[float]:\n",
        "    \"\"\"Main MBPP+ reward: test pass rate.\"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    # Handle test and test_imports batching\n",
        "    if isinstance(test, str):\n",
        "        tests = [test] * len(completions)\n",
        "    else:\n",
        "        try:\n",
        "            test_list = list(test)\n",
        "            if len(test_list) == len(completions):\n",
        "                tests = test_list\n",
        "            else:\n",
        "                tests = test_list * len(completions) if len(test_list) > 0 else [test_list[0]] * len(completions)\n",
        "        except:\n",
        "            tests = [test] * len(completions)\n",
        "\n",
        "    if test_imports is None:\n",
        "        imports_list = [None] * len(completions)\n",
        "    elif isinstance(test_imports, str):\n",
        "        imports_list = [test_imports] * len(completions)\n",
        "    else:\n",
        "        try:\n",
        "            test_imports_list = list(test_imports)\n",
        "            if len(test_imports_list) == len(completions):\n",
        "                imports_list = test_imports_list\n",
        "            else:\n",
        "                imports_list = [test_imports_list] * len(completions)\n",
        "        except:\n",
        "            imports_list = [test_imports] * len(completions)\n",
        "\n",
        "    # Evaluate each completion\n",
        "    for i, completion in enumerate(completions):\n",
        "        code = extract_code(completion)\n",
        "        if code is None:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            passed, total = execute_test(code, tests[i], imports_list[i], timeout=3.0)\n",
        "            reward = passed / total if total > 0 else 0.0\n",
        "            rewards.append(reward)\n",
        "        except Exception:\n",
        "            rewards.append(0.0)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def code_format_reward(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"Format reward: encourage code blocks.\"\"\"\n",
        "    return [0.5 if has_code_block(c) else -0.2 for c in completions]\n",
        "\n",
        "DEFAULT_REWARD_FNS_MBPP = [\n",
        "    mbppplus_verifier_reward,\n",
        "    code_format_reward,\n",
        "]\n",
        "\n",
        "print(\"✓ Reward functions defined\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T01:32:33.680184Z",
          "iopub.execute_input": "2025-12-26T01:32:33.680444Z",
          "iopub.status.idle": "2025-12-26T01:32:33.691611Z",
          "shell.execute_reply.started": "2025-12-26T01:32:33.680413Z",
          "shell.execute_reply": "2025-12-26T01:32:33.690812Z"
        },
        "id": "C5AVNEPt6UoI",
        "outputId": "2647e849-9728-4740-859a-c99bef0fe7f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Reward functions defined\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Loader Function"
      ],
      "metadata": {
        "id": "07oKjpTq6UoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import grain.python as grain\n",
        "import pyarrow\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "\n",
        "def get_mbpp_dataset(\n",
        "    local_path=\"./data/mbppplus_hf\",\n",
        "    train_fraction=0.9,\n",
        "    batch_size=1,\n",
        "    num_train_batches=None,\n",
        "    num_test_batches=64,\n",
        "    num_epochs=1,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Load MBPP+ dataset using grain.\"\"\"\n",
        "    import glob\n",
        "\n",
        "    # Load parquet\n",
        "    parquet_files = glob.glob(f\"{local_path}/*.parquet\")\n",
        "    if not parquet_files:\n",
        "        raise FileNotFoundError(f\"No parquet files found in {local_path}\")\n",
        "\n",
        "    table = pq.read_table(parquet_files[0])\n",
        "    dataset = table.to_pylist()\n",
        "\n",
        "    # Format prompts\n",
        "    for item in dataset:\n",
        "        item['prompts'] = f\"\"\"# Problem: {item['prompt']}\n",
        "# Write a Python function to solve this problem.\n",
        "# Return only Python code in a ```python ... ``` block.\n",
        "\n",
        "\"\"\"\n",
        "        # Convert test_imports list to string\n",
        "        if 'test_imports' in item and item['test_imports']:\n",
        "            if isinstance(item['test_imports'], list):\n",
        "                item['test_imports'] = '\\n'.join(item['test_imports'])\n",
        "\n",
        "    # Split dataset\n",
        "    total_samples = len(dataset)\n",
        "    train_size = int(total_samples * train_fraction)\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(seed)\n",
        "        indices = np.random.permutation(total_samples)\n",
        "        dataset = [dataset[i] for i in indices]\n",
        "\n",
        "    train_data = dataset[:train_size]\n",
        "    test_data = dataset[train_size:] if train_size < total_samples else dataset[-2:]\n",
        "\n",
        "    # Create grain datasets\n",
        "    train_source = grain.MapDataset.source(train_data)\n",
        "    test_source = grain.MapDataset.source(test_data)\n",
        "\n",
        "    # Apply transformations\n",
        "    train_ds = train_source.batch(batch_size=batch_size)\n",
        "    test_ds = test_source.batch(batch_size=batch_size)\n",
        "\n",
        "    if num_train_batches:\n",
        "        train_ds = train_ds[:num_train_batches]\n",
        "    if num_test_batches:\n",
        "        test_ds = test_ds[:num_test_batches]\n",
        "\n",
        "    train_ds = train_ds.repeat(num_epochs)\n",
        "\n",
        "    dataset_lengths = (len(train_ds), 0, len(test_ds))\n",
        "\n",
        "    return train_ds, None, test_ds, dataset_lengths\n",
        "\n",
        "print(\"✓ Data loader defined\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T03:39:22.644004Z",
          "iopub.execute_input": "2025-12-26T03:39:22.644301Z",
          "iopub.status.idle": "2025-12-26T03:39:23.062367Z",
          "shell.execute_reply.started": "2025-12-26T03:39:22.644282Z",
          "shell.execute_reply": "2025-12-26T03:39:23.061123Z"
        },
        "id": "VI9VmOAx6UoI",
        "outputId": "cb5c6c69-30fd-4666-e055-7fb6ed65f8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data loader defined\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Download MBPP+ Dataset"
      ],
      "metadata": {
        "id": "gZ7xE8qh6UoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(\"pyarrow.__version__\", pyarrow.__version__)\n",
        "print(\"datasets.__version__\", datasets.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WkQvRI_K6UoJ",
        "outputId": "e283a993-4990-45c4-ee0f-7df9cd330c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyarrow.__version__ 22.0.0\n",
            "datasets.__version__ 4.4.2\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!uv add numpy==1.26.4\n",
        "\n",
        "import os\n",
        "import numpy\n",
        "print(numpy.__version__)\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(pyarrow.__version__)\n",
        "print(\"Downloading MBPP+ dataset...\")\n",
        "os.makedirs(\"./data/mbppplus_hf\", exist_ok=True)\n",
        "dataset = load_dataset(\"evalplus/mbppplus\", split=\"test\")\n",
        "dataset.to_parquet(\"./data/mbppplus_hf/test.parquet\")\n",
        "print(f\"✓ Downloaded {len(dataset)} samples\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T03:39:27.332567Z",
          "iopub.execute_input": "2025-12-26T03:39:27.333136Z",
          "iopub.status.idle": "2025-12-26T03:39:29.539686Z",
          "shell.execute_reply.started": "2025-12-26T03:39:27.333114Z",
          "shell.execute_reply": "2025-12-26T03:39:29.53828Z"
        },
        "id": "sdlshIDZ6UoJ",
        "outputId": "a8f79762-9fc6-47e5-ea99-ae68ca39a72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "6398f94d97cd40ea956e6faba3571248",
            "0404b011e26a45bda3a343c933de7f15",
            "b440f4107759428bb87a1387ca965dbc",
            "7082feafc8914f0d902a10151435e5ee",
            "a291e67142da4349abe852d10a703c43",
            "b7d19c857ef442d9a127a1ba8bd7395c",
            "faa3b1d70b3047ecb4278097d35f66d7",
            "5ee0f09ad4f14e01add55b75d157e1df",
            "1928f9b91d0e4a34aea3c2838abe629e",
            "16babec0f8a94e148bf4ac5bd6f52274",
            "12699d8a8fb8423081aff21afaecc0d1",
            "e6fa2dcb3583422e9dc0da6d887cf5a4",
            "938999d492ab424b8c2882ad633e5b18",
            "c067a39f7c5e4db6b5063aa2dee1dd55",
            "e768ba6dd1fc48488379d41040bd8bee",
            "0f1f934ad21d4ca984d1c0d8e7d727b0",
            "6f43875ec228459fabf516ccebe0279f",
            "4c2dc85576fd48a09109e8c63fc59eca",
            "6ad848d1d23b4a268edecf3966639cc6",
            "7d5a914c980f4090a21de68ac55c471b",
            "29622b40becb439da38dc095f059e804",
            "b4a19eb0f7694c40b64ed5483a4d33a9",
            "e8fc998405ec45ceb944bdb97829dea9",
            "eff81898aea14fd2b1dc5972e22efdc3",
            "4547027b20b6455b8ed858b882c3b82f",
            "3c8a484e3ff44f709db101fd416ec1b0",
            "d5fcb5578cf545ac99874d602e4e866d",
            "9ae139504d9a4f5491761afa0471d085",
            "4b6fbfac189a4c13a43040cf5a83b483",
            "a9fca2f59bc54a71aaf97303139a4162",
            "41085ded10304cd791f2b03dbeb78e6c",
            "6c5e5f750d44459599f986fe609b95b1",
            "34c332537e554aaebf859860f4f6eaa6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.9.18)\n",
            "Found existing installation: numpy 2.3.5\n",
            "Uninstalling numpy-2.3.5:\n",
            "  Successfully uninstalled numpy-2.3.5\n",
            "\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 0.79ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.04ms\u001b[0m\u001b[0m\n",
            "2.0.2\n",
            "22.0.0\n",
            "Downloading MBPP+ dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001-d5781c9c51e0279(…):   0%|          | 0.00/1.13M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6398f94d97cd40ea956e6faba3571248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/378 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6fa2dcb3583422e9dc0da6d887cf5a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8fc998405ec45ceb944bdb97829dea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Downloaded 378 samples\n",
            "CPU times: user 314 ms, sys: 106 ms, total: 421 ms\n",
            "Wall time: 3.54 s\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cnHH2dIw6UoJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Import Libraries and Setup"
      ],
      "metadata": {
        "id": "CjzYHFtc6UoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from flax import nnx\n",
        "import optax\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "from tunix.models.gemma3 import model as gemma_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "\n",
        "print(\"✓ All libraries imported\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T03:38:12.702425Z",
          "iopub.execute_input": "2025-12-26T03:38:12.702564Z",
          "iopub.status.idle": "2025-12-26T03:38:28.544178Z",
          "shell.execute_reply.started": "2025-12-26T03:38:12.702548Z",
          "shell.execute_reply": "2025-12-26T03:38:28.543073Z"
        },
        "id": "zd_eBr4l6UoJ",
        "outputId": "847e5903-5dea-49ad-a330-d7b8e37d4246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All libraries imported\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Setup TPU Mesh"
      ],
      "metadata": {
        "id": "YJ-IV57z6UoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TPUS = len(jax.devices())\n",
        "print(NUM_TPUS)\n",
        "MESH_COUNTS = (2, 4)\n",
        "MESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n",
        "\n",
        "devices = jax.devices()\n",
        "device_type = devices[0].platform\n",
        "num_devices = len(devices)\n",
        "\n",
        "print(f\"Device type: {device_type}\")\n",
        "print(f\"Number of devices: {num_devices}\")\n",
        "\n",
        "import numpy as np\n",
        "if num_devices == 8:\n",
        "    print(\"Using 2D mesh: (1, 8)\")\n",
        "    devices_2d = np.array(devices).reshape(1, 8)\n",
        "    mesh = jax.make_mesh(\n",
        "        *MESH,\n",
        "        axis_types=(jax.sharding.AxisType.Auto,) * len(MESH_COUNTS),\n",
        "    )\n",
        "elif num_devices == 1:\n",
        "    print(\"Using 2D mesh: (1, 1)\")\n",
        "    devices_2d = np.array(devices).reshape(1, 1)\n",
        "    mesh = jax.sharding.Mesh(devices_2d, axis_names=('fsdp', 'tp'))\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported device count: {num_devices}\")\n",
        "\n",
        "print(f\"✓ Mesh created: {mesh}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T03:39:08.318539Z",
          "iopub.execute_input": "2025-12-26T03:39:08.318843Z",
          "iopub.status.idle": "2025-12-26T03:39:08.324849Z",
          "shell.execute_reply.started": "2025-12-26T03:39:08.318823Z",
          "shell.execute_reply": "2025-12-26T03:39:08.323819Z"
        },
        "id": "y-wmK9Ia6UoJ",
        "outputId": "bcc998dd-1a5f-4279-8045-08afe0c254ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Device type: tpu\n",
            "Number of devices: 1\n",
            "Using 2D mesh: (1, 1)\n",
            "✓ Mesh created: Mesh('fsdp': 1, 'tp': 1, axis_types=(Auto, Auto))\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Load Dataset (10 samples)"
      ],
      "metadata": {
        "id": "NTvrMsOM6UoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Loading 10 MBPP+ samples...\")\n",
        "train_dataset, val_dataset, test_dataset, dataset_lengths = get_mbpp_dataset(\n",
        "    local_path=\"./data/mbppplus_hf\",\n",
        "    train_fraction=1.0,\n",
        "    batch_size=1,\n",
        "    num_train_batches=10,\n",
        "    num_test_batches=2,\n",
        "    num_epochs=1,\n",
        "    shuffle=False,\n",
        ")\n",
        "print(f\"✓ Loaded {dataset_lengths[0]} training batches\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T03:39:13.057276Z",
          "iopub.execute_input": "2025-12-26T03:39:13.057664Z",
          "iopub.status.idle": "2025-12-26T03:39:13.213409Z",
          "shell.execute_reply.started": "2025-12-26T03:39:13.057645Z",
          "shell.execute_reply": "2025-12-26T03:39:13.212262Z"
        },
        "id": "mSYeF_6y6UoJ",
        "outputId": "12ada972-b44c-4740-f04d-d0e5ad6a9234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 10 MBPP+ samples...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No parquet files found in ./data/mbppplus_hf",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2655159573.py\u001b[0m in \u001b[0;36mget_mbpp_dataset\u001b[0;34m(local_path, train_fraction, batch_size, num_train_batches, num_test_batches, num_epochs, shuffle, seed)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mparquet_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{local_path}/*.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparquet_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No parquet files found in {local_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No parquet files found in ./data/mbppplus_hf"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Load Gemma-3-1B Model"
      ],
      "metadata": {
        "id": "-17n0RL76UoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "token = getpass(\"Enter your Hugging Face token (will not be shown): \")\n",
        "os.environ[\"HF_TOKEN\"] = token"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T01:49:50.722217Z",
          "iopub.execute_input": "2025-12-26T01:49:50.722479Z",
          "iopub.status.idle": "2025-12-26T01:49:50.72684Z",
          "shell.execute_reply.started": "2025-12-26T01:49:50.722458Z",
          "shell.execute_reply": "2025-12-26T01:49:50.725447Z"
        },
        "id": "gCoQ02X36UoK",
        "outputId": "cab90e2a-3f5c-4a63-ce75-207398e7819f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "8\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemma_lib.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nsMHecDc6UoK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import copy\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "print(\"Loading Gemma-3-1B model...\")\n",
        "model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
        "\n",
        "# Download model\n",
        "print(\"  Downloading from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=MODEL_ID,\n",
        "    ignore_patterns=[\"*.pth\"],\n",
        "    token=token\n",
        ")\n",
        "print(f\"  Model at: {local_model_path}\")\n",
        "\n",
        "# Create model\n",
        "print(\"  Creating model on mesh...\")\n",
        "with mesh:\n",
        "    actor_model = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "        local_model_path, model_config, mesh\n",
        "    )\n",
        "    ref_model = copy.deepcopy(actor_model)\n",
        "\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad = False\n",
        "print(\"✓ Model loaded\")\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = tokenizer_lib.Tokenizer(\n",
        "    tokenizer_path=TOKENIZER_PATH,\n",
        "    tokenizer_type='sentencepiece'\n",
        ")\n",
        "print(\"✓ Tokenizer loaded\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T01:52:17.290481Z",
          "iopub.execute_input": "2025-12-26T01:52:17.290805Z",
          "iopub.status.idle": "2025-12-26T01:52:42.607175Z",
          "shell.execute_reply.started": "2025-12-26T01:52:17.290785Z",
          "shell.execute_reply": "2025-12-26T01:52:42.606014Z"
        },
        "id": "-fSM4M6R6UoK",
        "outputId": "3224467f-a41e-4c74-c020-9ad93ddc9675",
        "colab": {
          "referenced_widgets": [
            "5004e3926e4e4e46b3a26b580fa063c8",
            "adf482f32edb4c19b96c7ff860f7d7b7"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading Gemma-3-1B model...\n  Downloading from Hugging Face...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (incomplete total...): 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5004e3926e4e4e46b3a26b580fa063c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adf482f32edb4c19b96c7ff860f7d7b7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "  Model at: /root/.cache/huggingface/hub/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\n  Creating model on mesh...\n✓ Model loaded\n✓ Tokenizer loaded\nCPU times: user 19.8 s, sys: 2.84 s, total: 22.6 s\nWall time: 25.3 s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Create RL Cluster"
      ],
      "metadata": {
        "id": "-r8hXtGZ6UoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optax.adamw(learning_rate=LEARNING_RATE)\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=10,\n",
        "        max_steps=10,\n",
        "        mini_batch_size=1,\n",
        "        train_micro_batch_size=1,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_SEQ_LEN,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K,\n",
        "        top_p=TOP_P,\n",
        "    ),\n",
        ")\n",
        "\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=actor_model,\n",
        "    reference=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "print(\"✓ RL Cluster created\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "rUzknQBs6UoK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Run GRPO Training"
      ],
      "metadata": {
        "id": "mJ2Hxk3-6UoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Creating GRPO trainer...\")\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        ")\n",
        "\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=DEFAULT_REWARD_FNS_MBPP,\n",
        "    algo_config=grpo_config,\n",
        ")\n",
        "print(f\"Config: {NUM_GENERATIONS} generations, beta={BETA}, epsilon={EPSILON}\")\n",
        "print(f\"Reward functions: {len(DEFAULT_REWARD_FNS_MBPP)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Starting GRPO training on 10 samples...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "grpo_trainer.train(\n",
        "    train_ds=train_dataset,\n",
        "    eval_ds=test_dataset,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ Training completed successfully!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T01:52:48.027333Z",
          "iopub.execute_input": "2025-12-26T01:52:48.02759Z",
          "iopub.status.idle": "2025-12-26T01:54:55.395172Z",
          "shell.execute_reply.started": "2025-12-26T01:52:48.027573Z",
          "shell.execute_reply": "2025-12-26T01:54:55.393808Z"
        },
        "id": "btPXepTQ6UoK",
        "outputId": "13d0f329-a817-483c-c2df-807a27686b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Creating GRPO trainer...\nConfig: 2 generations, beta=0.08, epsilon=0.2\nReward functions: 2\n\n================================================================================\nStarting GRPO training on 10 samples...\n================================================================================\nCPU times: user 2min 7s, sys: 3.64 s, total: 2min 11s\nWall time: 2min 6s\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCreating GRPO trainer...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mgrpo_config = GRPOConfig(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    num_generations=NUM_GENERATIONS,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    beta=BETA,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    epsilon=EPSILON,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    num_iterations=NUM_ITERATIONS,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mgrpo_trainer = GRPOLearner(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    rl_cluster=rl_cluster,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    reward_fns=DEFAULT_REWARD_FNS_MBPP,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    algo_config=grpo_config,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConfig: \u001b[39;49m\u001b[38;5;132;43;01m{NUM_GENERATIONS}\u001b[39;49;00m\u001b[33;43m generations, beta=\u001b[39;49m\u001b[38;5;132;43;01m{BETA}\u001b[39;49;00m\u001b[33;43m, epsilon=\u001b[39;49m\u001b[38;5;132;43;01m{EPSILON}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReward functions: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mlen(DEFAULT_REWARD_FNS_MBPP)}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m + \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m * 80)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStarting GRPO training on 10 samples...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m * 80)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mgrpo_trainer.train(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_ds=train_dataset,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    eval_ds=test_dataset,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m + \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m * 80)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m✅ Training completed successfully!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m * 80)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:21\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/grpo/grpo_learner.py:398\u001b[39m, in \u001b[36mGRPOLearner.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m    352\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    353\u001b[39m     train_ds: Iterable[TrainingInputT],\n\u001b[32m    354\u001b[39m     eval_ds: Iterable[TrainingInputT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    355\u001b[39m     skip_jit: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    356\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    357\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"GRPO training loop.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03m  Algorithm as below: extract from https://arxiv.org/abs/2402.03300 ::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m    skip_jit: Whether to skip JIT compilation of the training loop.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:606\u001b[39m, in \u001b[36mRLLearner.train\u001b[39m\u001b[34m(self, train_ds, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    603\u001b[39m initial_steps = \u001b[38;5;28mself\u001b[39m._iter_steps\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rl_cluster.perf.span_group(\u001b[33m\"\u001b[39m\u001b[33mglobal_step\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_global_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfull_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m      \u001b[49m\u001b[43mservice_target_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m      \u001b[49m\u001b[43mgrad_acc_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m      \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m      \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.should_sync_weights:\n\u001b[32m    617\u001b[39m     logging.debug(\n\u001b[32m    618\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSyncing weights at global step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.rl_cluster.global_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mini batch step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._iter_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    621\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:663\u001b[39m, in \u001b[36mRLLearner._run_global_step\u001b[39m\u001b[34m(self, full_batch_size, mini_batch_size, service_target_batch_size, grad_acc_steps, train_iterator, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    660\u001b[39m initial_steps = \u001b[38;5;28mself\u001b[39m._iter_steps\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rl_cluster.perf.span_group(\u001b[33m\"\u001b[39m\u001b[33mmini_batch_step\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_mini_batch_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m      \u001b[49m\u001b[43minitial_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m      \u001b[49m\u001b[43mservice_target_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m      \u001b[49m\u001b[43mgrad_acc_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m      \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m      \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# sync the iter steps with internel trainer, this is based on the\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[38;5;66;03m# assumption that the trainer internally doesn't reset the iter steps.\u001b[39;00m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# there is current a unit test to ensure this assumption.\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[38;5;28mself\u001b[39m._iter_steps = \u001b[38;5;28mself\u001b[39m.rl_cluster.actor_trainer.iter_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:688\u001b[39m, in \u001b[36mRLLearner._run_mini_batch_step\u001b[39m\u001b[34m(self, initial_steps, service_target_batch_size, grad_acc_steps, train_iterator, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    686\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run one mini batch step.\"\"\"\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rl_cluster.perf.span_group(\u001b[33m\"\u001b[39m\u001b[33mmicro_batch_steps\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_all_micro_batch_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m      \u001b[49m\u001b[43minitial_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mservice_target_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m      \u001b[49m\u001b[43mgrad_acc_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m      \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m      \u001b[49m\u001b[43mskip_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:780\u001b[39m, in \u001b[36mRLLearner._run_all_micro_batch_steps\u001b[39m\u001b[34m(self, initial_steps, service_target_batch_size, grad_acc_steps, train_iterator, eval_ds, skip_jit)\u001b[39m\n\u001b[32m    773\u001b[39m       \u001b[38;5;28mself\u001b[39m.rl_cluster.update_critic(\n\u001b[32m    774\u001b[39m           curr_train_ds,\n\u001b[32m    775\u001b[39m           curr_eval_ds,\n\u001b[32m    776\u001b[39m           skip_jit,\n\u001b[32m    777\u001b[39m       )  \u001b[38;5;66;03m# loop over μ num_iterations\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# call to throw stop iteration as a signal to break the loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:499\u001b[39m, in \u001b[36mRLLearner._prepare_data\u001b[39m\u001b[34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, service_target_batch_size, data_queue, async_loading, mode)\u001b[39m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    501\u001b[39m   \u001b[38;5;66;03m# Signal no more iterable to be loaded.\u001b[39;00m\n\u001b[32m    502\u001b[39m   data_queue.put(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:465\u001b[39m, in \u001b[36mRLLearner._prepare_data\u001b[39m\u001b[34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, service_target_batch_size, data_queue, async_loading, mode)\u001b[39m\n\u001b[32m    463\u001b[39m produced_training_examples = []\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accumulated_samples_num >= service_target_batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m   produced_training_examples = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_accumulated_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmicro_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmicro_batch_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m   micro_batches.clear()\n\u001b[32m    471\u001b[39m   micro_batch_sizes.clear()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_learner.py:270\u001b[39m, in \u001b[36mRLLearner._process_accumulated_batches\u001b[39m\u001b[34m(self, micro_batches, micro_batch_sizes, mode)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# Merge multiple training micro-batches\u001b[39;00m\n\u001b[32m    268\u001b[39m merged = rl_utils.merge_micro_batches(micro_batches)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m combined_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_compute_advantage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Split back to original training micro size\u001b[39;00m\n\u001b[32m    273\u001b[39m produced: \u001b[38;5;28mlist\u001b[39m[common.TrainExample] = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/grpo/grpo_learner.py:227\u001b[39m, in \u001b[36mGRPOLearner._generate_and_compute_advantage\u001b[39m\u001b[34m(self, training_input, mode)\u001b[39m\n\u001b[32m    224\u001b[39m completion_mask = completion_mask * completion_padding_mask\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.algo_config.beta != \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m   devices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrl_cluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mr2m\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl_cluster_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRole\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFERENCE\u001b[49m\u001b[43m]\u001b[49m.devices\n\u001b[32m    228\u001b[39m   \u001b[38;5;66;03m# TODO(yangmu): use function decorator to trace this part, same below.\u001b[39;00m\n\u001b[32m    229\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rl_cluster.perf.span(\u001b[33m\"\u001b[39m\u001b[33mrefer_inference\u001b[39m\u001b[33m\"\u001b[39m, devices) \u001b[38;5;28;01mas\u001b[39;00m interval:\n",
            "\u001b[31mKeyError\u001b[39m: <Role.REFERENCE: 'reference'>"
          ],
          "ename": "KeyError",
          "evalue": "<Role.REFERENCE: 'reference'>",
          "output_type": "error"
        }
      ],
      "execution_count": null
    }
  ]
}